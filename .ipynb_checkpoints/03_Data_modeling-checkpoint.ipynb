{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADS LIBRARIES\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "#import dill as pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import time \n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interactive, interact, fixed\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode()\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import shap\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler, KBinsDiscretizer\n",
    "import category_encoders as ce\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import uniform\n",
    "from random import randint\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve,\\\n",
    "                            precision_recall_curve, roc_auc_score, balanced_accuracy_score, log_loss, \\\n",
    "                            mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, validation_curve,\\\n",
    "                                    TimeSeriesSplit, RandomizedSearchCV, GridSearchCV, StratifiedKFold,\\\n",
    "                                    train_test_split, KFold, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the storage directories\n",
    "OUTPUT_DIR = os.getcwd() + '/results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADS CONSOLIDATED FEEDBACK DF\n",
    "\n",
    "df_analysis = pd.read_pickle(OUTPUT_DIR + 'df_vhf_s2.pkl')\n",
    "\n",
    "print(df_analysis.shape)\n",
    "print('\\n')\n",
    "\n",
    "for c in df_analysis.columns.tolist():\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_analysis['h_hora'].value_counts(dropna=False).reset_index().sort_values(['index'])) \n",
    "print(df_analysis['aut_nombre'].value_counts(dropna=False).reset_index().sort_values(['index'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selects an stratified sample, to compute pbi graphics\n",
    "\n",
    "df_analysis_01a = df_analysis.groupby('h_hora', group_keys=False).apply(lambda x: x.sample(frac=0.3))\n",
    "print(df_analysis_01a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "print(df_analysis_01a['h_hora'].value_counts(dropna=False).reset_index().sort_values(['index'])) \n",
    "print(df_analysis_01a['disp_ubicacion'].value_counts(dropna=False).reset_index())\n",
    "print(df_analysis_01a['disp_nombre'].value_counts(dropna=False).reset_index()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exports the sample to a csv format\n",
    "df_analysis_01a.to_csv(OUTPUT_DIR + 'df_vhf_sample.csv',index=False, encoding='latin_1')\n",
    "print(df_analysis_01a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defines study variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_01 = df_analysis.copy()\n",
    "df_analysis_01.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here all numerical columns\n",
    "ls_cat_variables = [\n",
    " #'mes,_dã\\xada,_aã±o_de_h_fecha',\n",
    " 'aut_nombre',\n",
    " 'disp_nombre',\n",
    " 'seccion_sentido',\n",
    " 'h_hora',\n",
    " 'disp_ubicacion',\n",
    " 'mes',\n",
    " 'dia_semana',\n",
    "]\n",
    "\n",
    "# Here all categorical columns\n",
    "ls_num_variables = [\n",
    "]\n",
    "\n",
    "\n",
    "# Here all key columns\n",
    "ls_keys = [\n",
    "]\n",
    "\n",
    "Target =  'h_cant_veh' #specify the column containing the target\n",
    "#Target_desc = 'periodo_cancelacion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines proper types to variables groups\n",
    "df_analysis_01[ls_cat_variables] = df_analysis_01[ls_cat_variables].astype('category')\n",
    "df_analysis_01[ls_num_variables] = df_analysis_01[ls_num_variables].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis_01[ls_cat_variables + ls_num_variables].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defines X matrix and response vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines X vector to all process\n",
    "\n",
    "X = df_analysis_01[ls_cat_variables + ls_num_variables]\n",
    "print(X.shape)\n",
    "\n",
    "y = df_analysis_01[Target]\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y.value_counts(dropna=False).reset_index().sort_values(['index'],ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates the split sets and prepares the cross/vaidation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the main splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# keep an independant validation set\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Stratified Cross folds.\n",
    "#skf = StratifiedKFold(n_splits=3, random_state=42, shuffle=True) #Here it does not apply because it is not a class estimatior\n",
    "kf = KFold(n_splits=3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepares inputers, escalers, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stablishes the imputers to be considered\n",
    "\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean',copy=True)\n",
    "median_imputer = SimpleImputer(missing_values=np.nan, strategy='median',copy=False)\n",
    "most_frequent_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent',copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stablishes the columns encoder to be considered\n",
    "\n",
    "count_enc = ce.CountEncoder(handle_unknown='value', normalize=False, min_group_size=False)\n",
    "onehot_enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "ordinal_enc = OrdinalEncoder()\n",
    "target_enc = ce.TargetEncoder()\n",
    "catboost_enc = ce.CatBoostEncoder()\n",
    "bins_enc = KBinsDiscretizer(n_bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoders The different kind of categorical encoders are explained before. Don't hesitate to check these two links :\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "\n",
    "The most commonly used ones are: Ordinal encoder and One-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stablishes scalers\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "#Stablishes pca variables reduction\n",
    "\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_num_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_cat_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the column trasformer\n",
    "# This will be the standard schema : start with the pipeline for column transformers and then fit the model\n",
    "\n",
    "#First a lil pip to treat imputation and scalation in one shot for numerical variables\n",
    "\n",
    "imputer_scaler_pip = Pipeline(steps=[('mean_imputer', mean_imputer),\n",
    "                                      ('scaler', StandardScaler())])\n",
    "\n",
    "onehot_pca_pip = Pipeline(steps=[('onehot', onehot_enc),\n",
    "                                ('pca',PCA(n_components=3)),])\n",
    "\n",
    "# Defines the column transformer\n",
    "xgb_transformer = ColumnTransformer([#(\"passthrough\", \"passthrough\", ls_num_variables),\n",
    "                                        ('onehot',onehot_enc, ls_cat_variables), #onehot + pca                                        \n",
    "                                        ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepares model pip\n",
    "\n",
    "xgb_pip = Pipeline([('preprocessing', xgb_transformer),\n",
    "                    ('model', XGBRegressor())])\n",
    "\n",
    "\n",
    "#Fits the pipeline to the data\n",
    "xgb_proc = xgb_pip.fit(X,y)\n",
    "\n",
    "#Saves the total procedure\n",
    "#pickle.dump(kmeans_proc, open( DIRECTORY + 'kmeans_proc.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid of model hyperparameters to be optimised, you can look for parameters definitions in the model link\n",
    "xgb_params = {'model__n_estimators': [50,100], #Number of trees\n",
    "              'model__max_depth': range(2, 8),\n",
    "              'model__eta': [0.3, 0.1, 0.01],\n",
    "              'model__subsample': [0.3, 0.5, 0.7, 1],\n",
    "              'model__colsample_bytree': [0.3, 0.5, 0.7, 1],\n",
    "              \n",
    "              'model__metric': ['squaredlogerror'],\n",
    "              'model__eval_metic': ['squaredlogerror'],\n",
    "              \n",
    "              'model__early_stopping_rounds': [42],\n",
    "              'model__eval_set' : [[X_test, y_test]]\n",
    "\n",
    "             }\n",
    "\n",
    "\n",
    "# We launch a grid seach: testing different hyperparameters on a cross validation \n",
    "# and select the ones that optimises the performance of the mode \n",
    "\n",
    "xgb_search = RandomizedSearchCV(xgb_pip, xgb_params, \n",
    "                                n_iter=2, # This is the number of parameters to test , you can choose more\n",
    "                                n_jobs=6, # Number of jobs /threads\n",
    "                                verbose=2, \n",
    "                                cv=kf, # the stratified crossvalidation that we created before\n",
    "                                scoring='neg_mean_absolute_error')\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters :', xgb_search.best_params_)\n",
    "print('Best score (Log loss) :', xgb_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGHTGBM grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the column trasformer\n",
    "# This will be the standard schema : start with the pipeline for column transformers and then fit the model\n",
    "\n",
    "\n",
    "# Defines the column transformer\n",
    "lgbm_transformer = ColumnTransformer([#(\"passthrough\", \"passthrough\", ls_num_variables),\n",
    "                                      ('onehot',onehot_enc, ls_cat_variables), #onehot + pca\n",
    "                                        ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepares model pip\n",
    "\n",
    "lgbm_pip = Pipeline([('preprocessing', lgbm_transformer),\n",
    "                    ('model', LGBMRegressor())])\n",
    "\n",
    "#Fits the pipeline to the data\n",
    "lgbm_proc = lgbm_pip.fit(X,y)\n",
    "\n",
    "#Saves the total procedure\n",
    "#pickle.dump(kmeans_proc, open( DIRECTORY + 'kmeans_proc.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid of model hyperparameters to be optimised, you can look for parameters definitions in the model link\n",
    "lgbm_params = {'model__boosting_type': ['gbdt', 'goss'],\n",
    "               'model__n_estimators': [50,100,200],  # Number of trees\n",
    "               'model__max_depth': [2,3,4,5,6,7,8],\n",
    "               'model__eta': [0.3, 0.1, 0.01],\n",
    "               'model__subsample': [0.3, 0.5, 0.7, 1],\n",
    "               'model__colsample_bytree': [0.3, 0.5, 0.7, 1],\n",
    "               \n",
    "               'model__eval_metric': ['squaredlogerror'],\n",
    "               'model__metric': ['squaredlogerror'],\n",
    "               \n",
    "               #'model__early_stopping_round': [42],\n",
    "               'model__eval_set' : [[X_test, y_test]],\n",
    "    \n",
    "            }\n",
    "\n",
    "\n",
    "# We launch a grid seach: testing different hyperparameters on a cross validation \n",
    "# and select the ones that optimises the performance of the mode \n",
    "\n",
    "lgbm_search = RandomizedSearchCV(lgbm_pip, lgbm_params,\n",
    "                                 n_iter=10, # This is the number of parameters to test , you can choose more\n",
    "                                 n_jobs=6, \n",
    "                                 cv=kf, \n",
    "                                 scoring='neg_mean_absolute_error')\n",
    "\n",
    "lgbm_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters :', lgbm_search.best_params_)\n",
    "print('Best score (log_loss) :', lgbm_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LInear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the column trasformer\n",
    "# This will be the standard schema : start with the pipeline for column transformers and then fit the model\n",
    "\n",
    "\n",
    "# Defines the column transformer\n",
    "lr_transformer = ColumnTransformer([#(\"passthrough\", \"passthrough\", ls_num_variables),\n",
    "                                      ('onehot',onehot_enc, ls_cat_variables), #onehot + pca\n",
    "                                        ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepares model pip\n",
    "\n",
    "lr_pip = Pipeline([('preprocessing', lr_transformer),\n",
    "                    ('model', RFE(LinearRegression())),])\n",
    "\n",
    "#Fits the pipeline to the data\n",
    "lr_proc = lr_pip.fit(X,y)\n",
    "\n",
    "#Saves the total procedure\n",
    "#pickle.dump(kmeans_proc, open( DIRECTORY + 'kmeans_proc.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyper_params = {'model__n_features_to_select': [10,20,50,98]}\n",
    "\n",
    "lr_search = GridSearchCV(estimator = lr_pip, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = kf, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)\n",
    "\n",
    "# fit the model\n",
    "lr_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in lr_search.get_params().keys():\n",
    "#    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_transformer.fit_transform(X_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compares results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of models\n",
    "\n",
    "model_names = ['LightGBM', 'XGBoost', 'Linear']\n",
    "models = [lgbm_search, xgb_search, lr_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scores\n",
    "\n",
    "scores = []\n",
    "for model in models:\n",
    "    scores.append([r2_score(y_val, model.predict(X_val)),\n",
    "                   mean_absolute_error(y_val, model.predict(X_val))\n",
    "                  ])\n",
    "results = pd.DataFrame(scores, index=model_names, columns=['r2', 'mae'])\n",
    "results.sort_values(by='r2', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winner model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable importance\n",
    "\n",
    "result = permutation_importance(lgbm_search, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2, scoring= 'r2')\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_test.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in sklearn.metrics.SCORERS.keys():\n",
    "#    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap values\n",
    "\n",
    "# Create a random subset of data to acceleration computation of shap values and PdP\n",
    "np.random.seed(seed=1234)\n",
    "random_nb = np.random.choice(range(0, X_test.shape[0]), 1000)\n",
    "\n",
    "X_choose = X_test.iloc[random_nb].reset_index(drop=True)\n",
    "\n",
    "# Shap package struggles with pipeline, so we transform the X_choose here\n",
    "X_choose= lgbm_search.best_estimator_.named_steps['preprocessing'].transform(X_choose)\n",
    "print(X_choose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieves variable names for shap\n",
    "\n",
    "ls_catcols_nopca_tr = lgbm_search.best_estimator_['preprocessing'].transformers_[0][1].get_feature_names(ls_cat_variables).tolist()\n",
    "\n",
    "ls_shnames = ls_catcols_nopca_tr\n",
    "\n",
    "\n",
    "X_choose = pd.DataFrame(X_choose,columns=ls_shnames)\n",
    "\n",
    "print(X_choose.shape)\n",
    "X_choose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable importance shap values\n",
    "explainer = shap.TreeExplainer(lgbm_search.best_estimator_.named_steps['model'])\n",
    "shap_values = explainer.shap_values(X_choose)\n",
    "shap.summary_plot(shap_values, X_choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
